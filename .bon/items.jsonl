{"id": "consomme-gumotu", "type": "action", "title": "Test time-series profiling and forecast tool", "brief": {"why": "profiling-timeseries.md has 4 SQL patterns (grain detection, gap detection, daily distribution, quality checks) and forecast prep guidance — none tested against real data. The forecast MCP tool itself has never been called successfully.", "what": "1. Create or find a time-series dataset — needs: DATE column, numeric metric, at least 90 days of daily data, ideally with gaps and seasonality. Options: (a) GENERATE_DATE_ARRAY + RAND() synthetic table in mit-consomme-test.survey_data, (b) bigquery-public-data has google_trends, stackoverflow, github_repos with temporal data. 2. Run grain detection query from profiling-timeseries.md (MIN/MAX date, distinct dates, avg rows per date). 3. Run gap detection query (GENERATE_DATE_ARRAY LEFT JOIN). 4. Run daily distribution query. 5. Run quality assessment checks (late-arriving data, partial periods). 6. Test forecast tool: pre-aggregate to one row per date, call forecast with horizon=30, check output shape and plausibility. 7. Try forecast with id_cols for parallel series (e.g., per-category). 8. Fix any broken SQL patterns.", "done": "All 4 profiling-timeseries.md SQL patterns execute and return sensible results. forecast tool returns predictions with confidence bounds. At least one multi-series forecast tested. Any fixes applied to the reference file."}, "status": "open", "parent": "consomme-tetaka", "order": 1, "created_at": "2026-02-15T00:17:23Z", "created_by": "spm1001", "waiting_for": null}
{"id": "consomme-jupeji", "type": "action", "title": "Test warehouse profiling against multi-table schema", "brief": {"why": "profiling-warehouse.md covers FK discovery, column classification, completeness scoring, and consistency checks for star/snowflake schemas — none tested. This is the most common data shape in production BQ and the reference must work.", "what": "1. Create a small star schema in mit-consomme-test — e.g., split ohid_survey_raw into fact_responses + dim_respondent + dim_region + dim_question, or create a synthetic orders/customers/products schema. Needs: FK relationships, mixed column types, some nulls, some consistency issues to detect. 2. Run Phase 1 structural queries — row count, grain, PK uniqueness. 3. Run Phase 2 column-level profiling — nulls, distinct counts, numeric distributions, string patterns. 4. Run Phase 3 relationship discovery — FK integrity check (orphan records query), hierarchy detection. 5. Run quality assessment — completeness scoring (green/yellow/orange/red), consistency checks, accuracy indicators (placeholder values). 6. Fix any broken SQL.", "done": "All profiling-warehouse.md patterns execute against a real multi-table schema. FK integrity query finds orphans (or confirms none). Completeness scoring produces sensible ratings. Any fixes applied."}, "status": "open", "parent": "consomme-tetaka", "order": 2, "created_at": "2026-02-15T00:17:33Z", "created_by": "spm1001", "waiting_for": null}
{"id": "consomme-limuko", "type": "outcome", "title": "Skill works identically across Amp and Claude Code", "brief": {"why": "All testing so far has been in Amp. Claude Code has different MCP server config (settings.json vs claude_desktop_config.json), different skill loading paths, and different hook behaviour. A user on Claude Code could hit platform-specific issues we haven't seen.", "what": "1. Start a Claude Code session in /home/modha/Repos/consomme 2. Verify skill appears in available skills list 3. Trigger with 'analyse this data' — confirm bq-analyst loads 4. Run the discovery→understand→analyse flow against mit-consomme-test.survey_data.ohid_survey_raw 5. Test data shape detection fires correctly (should identify survey data) 6. Run a cross-tab query and check SQL is valid BQ dialect 7. Build a Chart.js dashboard from query results 8. Note any differences in tool names, parameter handling, or output format between Amp and Claude Code 9. Add platform notes to SKILL.md Setup Requirements section if needed", "done": "Same 5-stage workflow produces same results in Claude Code as in Amp. Any platform-specific quirks documented in SKILL.md. Dashboard renders correctly."}, "status": "open", "order": 2, "created_at": "2026-02-15T00:17:45Z", "created_by": "spm1001"}
{"id": "consomme-niwopa", "type": "outcome", "title": "Skill is clean and shareable with other GCP analysts", "brief": {"why": "The skill currently has personal paths, test project IDs, and no setup README. For sharing on GitHub or with colleagues, it needs to be clean of PII, have clear installation instructions, and pass the skill-forge sharing scanner.", "what": "1. Run skill-forge sharing scanner: python3 ~/.claude/skills/skill-forge/scripts/scan.py skills/bq-analyst 2. Fix any PII, hardcoded paths, or personal references flagged 3. Improve CSO score from 79 toward 85+ — timing_gates (12/25) and action_verbs (5/10) are the weak spots, try stronger MANDATORY/FIRST language and a domain-verb opener 4. Write README.md for the skill covering: what it does, prerequisites (BQ MCP server setup, ADC auth), installation (symlink to ~/.claude/skills/), supported data shapes, reference file index 5. Ensure install.sh works from a fresh clone — test the symlink creation 6. Check Apache-2.0 attribution is correct in SKILL.md header comment 7. Add .gitignore for test-outputs/ and mise-fetch/ if not already present", "done": "scan.py returns clean (no high-risk findings). CSO >= 85. README.md exists with setup instructions. install.sh creates working symlinks from fresh clone. No personal data in any skill file."}, "status": "open", "order": 3, "created_at": "2026-02-15T00:17:58Z", "created_by": "spm1001"}
{"id": "consomme-tetaka", "type": "outcome", "title": "Every profiling reference is validated against real BigQuery data", "brief": {"why": "Round 1-2 proved SQL snippets that 'look right' can fail — profiling-survey.md had issues with binary multi-select and off-scale codes that only surfaced when run against real data. profiling-timeseries.md and profiling-warehouse.md have NEVER been executed against real data. The forecast tool is also untested.", "what": "1. Load or find time-series dataset in mit-consomme-test (options: synthetic daily metrics, or a public BQ dataset with dates) 2. Run every SQL pattern from profiling-timeseries.md — grain detection, gap detection, seasonality checks, quality assessment 3. Test forecast tool with the time-series data — pre-aggregate, set horizon, check results 4. Load or find multi-table star-schema dataset in mit-consomme-test (options: synthetic fact+dimension tables, or denormalise the survey data into a star schema) 5. Run every SQL pattern from profiling-warehouse.md — FK discovery, column classification, completeness scoring, consistency checks 6. Fix any SQL that fails or produces wrong results 7. Document results in TESTING.md Round 3 section", "done": "All SQL snippets in profiling-timeseries.md and profiling-warehouse.md execute successfully against real BQ data. forecast tool returns plausible projections. Any fixes documented. TESTING.md Round 3 section complete with pass/fail for each pattern."}, "status": "open", "order": 1, "created_at": "2026-02-15T00:17:11Z", "created_by": "spm1001"}
