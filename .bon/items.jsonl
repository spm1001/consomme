{"id": "consomme-gumotu", "type": "action", "title": "Test time-series profiling and forecast tool", "brief": {"why": "profiling-timeseries.md has 4 SQL patterns (grain detection, gap detection, daily distribution, quality checks) and forecast prep guidance — none tested against real data. The forecast MCP tool itself has never been called successfully.", "what": "1. Create or find a time-series dataset — needs: DATE column, numeric metric, at least 90 days of daily data, ideally with gaps and seasonality. Options: (a) GENERATE_DATE_ARRAY + RAND() synthetic table in mit-consomme-test.survey_data, (b) bigquery-public-data has google_trends, stackoverflow, github_repos with temporal data. 2. Run grain detection query from profiling-timeseries.md (MIN/MAX date, distinct dates, avg rows per date). 3. Run gap detection query (GENERATE_DATE_ARRAY LEFT JOIN). 4. Run daily distribution query. 5. Run quality assessment checks (late-arriving data, partial periods). 6. Test forecast tool: pre-aggregate to one row per date, call forecast with horizon=30, check output shape and plausibility. 7. Try forecast with id_cols for parallel series (e.g., per-category). 8. Fix any broken SQL patterns.", "done": "All 4 profiling-timeseries.md SQL patterns execute and return sensible results. forecast tool returns predictions with confidence bounds. At least one multi-series forecast tested. Any fixes applied to the reference file."}, "status": "open", "parent": "consomme-tetaka", "order": 1, "created_at": "2026-02-15T00:17:23Z", "created_by": "spm1001", "waiting_for": null}
{"id": "consomme-jupeji", "type": "action", "title": "Test warehouse profiling against multi-table schema", "brief": {"why": "profiling-warehouse.md covers FK discovery, column classification, completeness scoring, and consistency checks for star/snowflake schemas — none tested. This is the most common data shape in production BQ and the reference must work.", "what": "1. Create a small star schema in mit-consomme-test — e.g., split ohid_survey_raw into fact_responses + dim_respondent + dim_region + dim_question, or create a synthetic orders/customers/products schema. Needs: FK relationships, mixed column types, some nulls, some consistency issues to detect. 2. Run Phase 1 structural queries — row count, grain, PK uniqueness. 3. Run Phase 2 column-level profiling — nulls, distinct counts, numeric distributions, string patterns. 4. Run Phase 3 relationship discovery — FK integrity check (orphan records query), hierarchy detection. 5. Run quality assessment — completeness scoring (green/yellow/orange/red), consistency checks, accuracy indicators (placeholder values). 6. Fix any broken SQL.", "done": "All profiling-warehouse.md patterns execute against a real multi-table schema. FK integrity query finds orphans (or confirms none). Completeness scoring produces sensible ratings. Any fixes applied."}, "status": "open", "parent": "consomme-tetaka", "order": 2, "created_at": "2026-02-15T00:17:33Z", "created_by": "spm1001", "waiting_for": null}
{"id": "consomme-limiza", "type": "action", "title": "Validate sql-reference.md patterns against real BQ data", "brief": {"why": "sql-reference.md is 222 lines of BQ-specific SQL patterns (window functions, CTEs, cohort retention, funnel analysis, deduplication) that agents copy directly into queries. Round 1 validated statistical-analysis.md and profiling-survey.md patterns but sql-reference.md has NEVER been executed. The cohort retention and funnel analysis patterns are particularly complex — multi-CTE queries with window functions that could have subtle BQ dialect issues.", "what": "1. Pick a dataset with temporal + entity structure (the timeseries dataset from consomme-gumotu would work, or use bigquery-public-data) 2. Test date/time functions block: DATE_ADD, DATE_SUB, DATE_DIFF, DATE_TRUNC, EXTRACT, FORMAT_DATE 3. Test string functions: LOWER LIKE pattern, REGEXP_CONTAINS, SPLIT/ARRAY_TO_STRING 4. Test window functions: ROW_NUMBER, RANK, running total, moving average, LAG/LEAD, FIRST_VALUE/LAST_VALUE, percent of total 5. Test CTE pattern: multi-step query with 3+ named CTEs 6. Test cohort retention query (needs user_id + activity_date structure) 7. Test funnel analysis query (needs event-type data) 8. Test deduplication pattern (ROW_NUMBER PARTITION BY) 9. Fix any syntax errors or BQ dialect issues 10. Can be combined with consomme-gumotu if using the same temporal dataset", "done": "All SQL blocks in sql-reference.md execute without errors. Any BQ dialect fixes applied. Complex patterns (cohort, funnel) return plausible results against real data."}, "status": "open", "parent": "consomme-tetaka", "order": 3, "created_at": "2026-02-15T10:24:54Z", "created_by": "spm1001", "waiting_for": null}
{"id": "consomme-limuko", "type": "outcome", "title": "Skill works identically across Amp and Claude Code", "brief": {"why": "All testing so far has been in Amp. Claude Code has different MCP server config (settings.json vs claude_desktop_config.json), different skill loading paths, and different hook behaviour. A user on Claude Code could hit platform-specific issues we haven't seen.", "what": "1. Start a Claude Code session in /home/modha/Repos/consomme 2. Verify skill appears in available skills list 3. Trigger with 'analyse this data' — confirm bq-analyst loads 4. Run the discovery→understand→analyse flow against mit-consomme-test.survey_data.ohid_survey_raw 5. Test data shape detection fires correctly (should identify survey data) 6. Run a cross-tab query and check SQL is valid BQ dialect 7. Build a Chart.js dashboard from query results 8. Note any differences in tool names, parameter handling, or output format between Amp and Claude Code 9. Add platform notes to SKILL.md Setup Requirements section if needed", "done": "Same 5-stage workflow produces same results in Claude Code as in Amp. Any platform-specific quirks documented in SKILL.md. Dashboard renders correctly."}, "status": "done", "order": 2, "created_at": "2026-02-15T00:17:45Z", "created_by": "spm1001", "done_at": "2026-02-16T12:34:02Z"}
{"id": "consomme-niwopa", "type": "outcome", "title": "Skill is clean and shareable with other GCP analysts", "brief": {"why": "The skill currently has personal paths, test project IDs, and no setup README. For sharing on GitHub or with colleagues, it needs to be clean of PII, have clear installation instructions, and pass the skill-forge sharing scanner.", "what": "1. Run skill-forge sharing scanner: python3 ~/.claude/skills/skill-forge/scripts/scan.py skills/bq-analyst 2. Fix any PII, hardcoded paths, or personal references flagged 3. Improve CSO score from 79 toward 85+ — timing_gates (12/25) and action_verbs (5/10) are the weak spots, try stronger MANDATORY/FIRST language and a domain-verb opener 4. Write README.md for the skill covering: what it does, prerequisites (BQ MCP server setup, ADC auth), installation (symlink to ~/.claude/skills/), supported data shapes, reference file index 5. Ensure install.sh works from a fresh clone — test the symlink creation 6. Check Apache-2.0 attribution is correct in SKILL.md header comment 7. Add .gitignore for test-outputs/ and mise-fetch/ if not already present", "done": "scan.py returns clean (no high-risk findings). CSO >= 85. README.md exists with setup instructions. install.sh creates working symlinks from fresh clone. No personal data in any skill file."}, "status": "done", "order": 3, "created_at": "2026-02-15T00:17:58Z", "created_by": "spm1001", "done_at": "2026-02-16T12:34:02Z"}
{"id": "consomme-refafi", "type": "outcome", "title": "Investigate double BIGQUERY_PROJECT between consomme and BQ extensions", "brief": {"why": "Both extensions define BIGQUERY_PROJECT as a setting — users may need to enter it twice. May conflict or shadow.", "what": "Check if Gemini CLI merges env vars across extensions or isolates them. Test with both installed.", "done": "Documented how the env var works and updated consomme if needed"}, "status": "open", "order": 5, "created_at": "2026-02-16T12:34:13Z", "created_by": "spm1001"}
{"id": "consomme-teloti", "type": "action", "title": "Add mixed-shape disambiguation example to data shape detection", "brief": {"why": "Section 3 of SKILL.md has a detection table with 3 heuristic rows (warehouse, survey, timeseries) but no guidance for when signals overlap. Real-world example: ohid_survey_raw has a date column AND question codes — an agent could reasonably pick either survey or timeseries. The skill says 'ask the user' but gives no worked example of the disambiguation conversation. Round 2 testing confirmed the clean-signal case works; the ambiguous case is untested.", "what": "1. Add a 'Mixed Signals' subsection to Section 3 after the detection table 2. Include a worked example showing overlapping signals (e.g., survey table with date column, or warehouse with temporal fact table) 3. Show the disambiguation question the agent should ask: 'This table has both question codes and a date column — are you interested in survey responses or trends over time?' 4. Add a rule: 'Match the reference to the analytical question, not just the table structure' (this sentence exists but is buried — make it more prominent) 5. Keep it short — 10-15 lines max, Section 3 is meant to be a quick routing decision", "done": "Section 3 has a Mixed Signals example with an explicit disambiguation question template. An agent encountering a table with overlapping signals has a clear path to ask the user rather than guessing."}, "status": "done", "parent": "consomme-niwopa", "order": 1, "created_at": "2026-02-15T10:24:40Z", "created_by": "spm1001", "waiting_for": null, "done_at": "2026-02-16T12:34:02Z"}
{"id": "consomme-tetaka", "type": "outcome", "title": "Every profiling reference is validated against real BigQuery data", "brief": {"why": "Round 1-2 proved SQL snippets that 'look right' can fail — profiling-survey.md had issues with binary multi-select and off-scale codes that only surfaced when run against real data. profiling-timeseries.md and profiling-warehouse.md have NEVER been executed against real data. The forecast tool is also untested.", "what": "1. Load or find time-series dataset in mit-consomme-test (options: synthetic daily metrics, or a public BQ dataset with dates) 2. Run every SQL pattern from profiling-timeseries.md — grain detection, gap detection, seasonality checks, quality assessment 3. Test forecast tool with the time-series data — pre-aggregate, set horizon, check results 4. Load or find multi-table star-schema dataset in mit-consomme-test (options: synthetic fact+dimension tables, or denormalise the survey data into a star schema) 5. Run every SQL pattern from profiling-warehouse.md — FK discovery, column classification, completeness scoring, consistency checks 6. Fix any SQL that fails or produces wrong results 7. Document results in TESTING.md Round 3 section", "done": "All SQL snippets in profiling-timeseries.md and profiling-warehouse.md execute successfully against real BQ data. forecast tool returns plausible projections. Any fixes documented. TESTING.md Round 3 section complete with pass/fail for each pattern."}, "status": "open", "order": 1, "created_at": "2026-02-15T00:17:11Z", "created_by": "spm1001"}
{"id": "consomme-vejoli", "type": "outcome", "title": "Test /consomme-sheets with real Google Sheet on Mac", "brief": {"why": "Command was built and pushed but never tested end-to-end — TOML prompt references mise MCP which Gemini may not resolve by name", "what": "Run gemini extensions update, then /consomme-sheets with a real Sheet URL on Mac. Verify mise fetch triggers, CSV loads, profiling runs.", "done": "Command works end-to-end on Mac with a real Sheet"}, "status": "open", "order": 4, "created_at": "2026-02-16T12:34:11Z", "created_by": "spm1001"}
